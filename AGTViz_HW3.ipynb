{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Valery Stoeva*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Those how are making projects on artilces: half of you need to choose co-authorship graph, others - citation (but consider citation graph as undirected for simplicity)\n",
    "2. Use weighted graph so that it would give you ability to calculate embeddings in more appropriate way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for example, take a period $2000$-$2014$ as training period and $2015$-$2018$ as testing period)\n",
    "    * Pick pairs of nodes that **have appeared during training interval** but **had no links** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem.\n",
    "2. Construct feature space:\n",
    "    * Use at least 2 features based on neighborhood \n",
    "    * Use at least 2 fetures based on shortest path\n",
    "    * Use embedding representation of nodes' pairs (for example, node2vec)\n",
    "    * Use idea of time series features (with time lag)\n",
    "    * Use idea of change-point detection\n",
    "3. Choose at least $3$ classification algorithms and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция получения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"articlesgrap.cxqhtfp4sprs.eu-central-1.rds.amazonaws.com\",\n",
    "  user=\"wizard\",\n",
    "  passwd=\"12345678\",\n",
    "  database=\"TheData\"\n",
    ")\n",
    "\n",
    "def getAutGraph(by, fy):\n",
    "    '''get co-authors between years by and fy'''\n",
    "    mycursor = mydb.cursor()\n",
    "    sql = \"SELECT a.auid, b.auid, count(b.auid) as weight FROM TheData.isAuthor as a join TheData.isAuthor as b on a.artid=b.artid join TheData.Articles as c on a.artid=c.id where c.year between %s and %s and a.auid <> b.auid group by a.auid, b.auid order by a.auid desc;\"\n",
    "    val = (by, fy)\n",
    "    mycursor.execute(sql, val)\n",
    "    myresult = mycursor.fetchall()\n",
    "    mycursor.close()\n",
    "    return myresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем период 2010-2013 за training period и 2014-2015 за testing period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_select = getAutGraph(2010,2013)\n",
    "train_G = nx.Graph()\n",
    "train_G.add_weighted_edges_from(train_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_select = getAutGraph(2014,2015)\n",
    "test_G = nx.Graph()\n",
    "test_G.add_weighted_edges_from(test_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим общие вершины и создаем копии обоих графов, содержащие только общие вершины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nodes = train_G.nodes & test_G.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_delete_from_train = [node for node in train_G.nodes if node not in common_nodes]\n",
    "train_G_filtered = train_G.copy()\n",
    "train_G_filtered.remove_nodes_from(nodes_to_delete_from_train)\n",
    "\n",
    "nodes_to_delete_from_test = [node for node in test_G.nodes if node not in common_nodes]\n",
    "test_G_filtered = test_G.copy()\n",
    "test_G_filtered.remove_nodes_from(nodes_to_delete_from_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_select, test_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ребра, для которых нам нужно будет делать предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = list(nx.non_edges(train_G_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала посчитаем ответы и, для интереса, распределение ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы сильно несбалансированные, ну ничего.\n",
    "\n",
    "## Фичи на основе соседей\n",
    "\n",
    "Добавим по 2 фичи для вершины на основе ребер из вершины - количество и взвешенную сумму, и по таких же 2 фичи для вершины на основе ребер из соседей вершины. 5 фича для пары вершин - количество общих соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour_sum, neighbour_weighted_sum, neighbour_edges_sum, neighbour_edges_weighted_sum, common_neighbours = {}, {}, {}, {}, {}\n",
    "for node in train_G.nodes():\n",
    "    neighbour_sum[node] = len(train_G[node])\n",
    "    neighbour_weighted_sum[node] = sum(edge_dict['weight'] for edge_dict in train_G[node].values())\n",
    "    \n",
    "for node in train_G_filtered.nodes():\n",
    "    neighbour_edges_sum[node] = sum(neighbour_sum[neighbour] for neighbour in train_G[node])\n",
    "    neighbour_edges_weighted_sum[node] = sum(neighbour_weighted_sum[neighbour] for neighbour in train_G[node])\n",
    "\n",
    "for edge in edge_list:\n",
    "    common_neighbours[edge] = len(list(nx.common_neighbors(train_G, *edge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи на основе расстояния\n",
    "Взвешенное и невзвешенное. Для взвешенного в каестве весов используем обратное число общих статей, чтобы расстояние было осмысленным. (при использовании не обратных общих статей при поиске кратчайших путей получается какая-то странная величина)\n",
    "\n",
    "Тут небольшое допущение - будем искать данные пути не на исходном графе, а на отфильтрованном (только из общих вершин обоих периодов), т.к. сложность флойда-Уоршелла кубическая; но может это и верно - зачем учитывать авторов, которые не появляются в обоих периодах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, d in train_G_filtered.edges.items():\n",
    "    d['opp_weight'] = 1 / d['weight']\n",
    "\n",
    "distances = {}\n",
    "weighted_distances = {}\n",
    "\n",
    "\n",
    "for cc in nx.connected_component_subgraphs(train_G_filtered):\n",
    "    distances.update(nx.floyd_warshall(cc))\n",
    "    weighted_distances.update(nx.floyd_warshall(cc, weight='opp_weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи на основе изменений харастеристик во времени\n",
    "Для каждого года обучающей выборки и для каждой вершины найдем изменение ее степени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = {node: [] for node in train_G_filtered.nodes()}\n",
    "\n",
    "for year in range(2010, 2013 + 1):\n",
    "    temp_select = getAutGraph(year, year)\n",
    "    temp_G = nx.Graph()\n",
    "    temp_G.add_weighted_edges_from(temp_select)\n",
    "    for node in train_G_filtered.nodes():\n",
    "        if node in temp_G.nodes:\n",
    "            time_series[node].append(len(temp_G[node]))\n",
    "        else:\n",
    "            time_series[node].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи на основе точек изменения тренда\n",
    "Будем называть год точкой изменения, если за этот год было написано n статей, за предыдущий год k статей, и max(n,k)/(min(n,k)+1) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_changes = {node: [] for node in train_G_filtered.nodes()}\n",
    "\n",
    "for node, flags in trends_changes.items():\n",
    "    prev = 0\n",
    "    for delta in time_series[node]:\n",
    "        flags.append(int(max(prev, delta) / (1 + min(prev, delta)) > 2))\n",
    "        prev = delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec\n",
    "\n",
    "Неочевидно, какой из двух весов учитывать в Node2Vec'е, поэтому будем подавать невзвешанные ребра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('in.txt', 'w') as f:\n",
    "    for edge in train_G.edges():\n",
    "        f.write('{} {}\\n'.format(*edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "!python2 n2v/src/main.py --input in.txt --output out.txt --dimensions 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_vecs = {node: [] for node in train_G_filtered.nodes()}\n",
    "\n",
    "with open('out.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f.readlines():\n",
    "        split_line = line.split()\n",
    "        nodes_to_vecs[int(split_line[0])] = list(map(float, split_line[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем фичи в numpy-матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "X = []\n",
    "y = [edge in test_G_filtered.edges for edge in edge_list]\n",
    "\n",
    "for edge in edge_list:\n",
    "    features = []\n",
    "    node_1, node_2 = edge\n",
    "    \n",
    "    # фичи соседства\n",
    "    features.extend(\n",
    "        (\n",
    "            neighbour_sum[node_1],\n",
    "            neighbour_sum[node_2],\n",
    "            neighbour_weighted_sum[node_1],\n",
    "            neighbour_weighted_sum[node_2],\n",
    "            neighbour_edges_sum[node_1],\n",
    "            neighbour_edges_sum[node_2],\n",
    "            neighbour_edges_weighted_sum[node_1],\n",
    "            neighbour_edges_weighted_sum[node_2],\n",
    "            common_neighbours[edge],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # фичи расстояния\n",
    "    # да, корректно это будет работать только для деревьев, и, возможно, нейросетей\n",
    "    features.extend(\n",
    "        (\n",
    "            distances[node_1].get(node_2, -1),\n",
    "            weighted_distances[node_1].get(node_2, -1),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # time-series\n",
    "    features.extend(time_series[node_1])\n",
    "    features.extend(time_series[node_2])\n",
    "\n",
    "    # changing poins\n",
    "    features.extend(trends_changes[node_1])\n",
    "    features.extend(trends_changes[node_2])\n",
    "\n",
    "    # node2vec\n",
    "    features.extend(nodes_to_vecs[node_1])\n",
    "    features.extend(nodes_to_vecs[node_2])\n",
    "\n",
    "    X.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131088, 91)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_matrix = np.array(X)\n",
    "y_matrix = np.array(y)\n",
    "\n",
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Тут и во всех классификаторах(кроме байесовского) мы будем использовать сбалансированные веса для классов, т.к. классы несбалансированы - доля положительного всего лишь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016096057610155011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "lr_predicts = np.zeros(y_matrix.shape)\n",
    "\n",
    "for train_index, test_index in kf.split(X_matrix):\n",
    "    train_X, train_y = X_matrix[train_index], y_matrix[train_index]\n",
    "    test_X, test_y = X_matrix[test_index], y_matrix[test_index]\n",
    "    lr.fit(train_X, train_y)\n",
    "    lr_predicts[test_index] = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_predicts = np.zeros(y_matrix.shape)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "\n",
    "for train_index, test_index in kf.split(X_matrix):\n",
    "    train_X, train_y = X_matrix[train_index], y_matrix[train_index]\n",
    "    test_X, test_y = X_matrix[test_index], y_matrix[test_index]\n",
    "    rf.fit(train_X, train_y)\n",
    "    rf_predicts[test_index] = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (градиентный бустинг)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lightgmb_predicts = np.zeros(y_matrix.shape)\n",
    "\n",
    "gb = LGBMClassifier(objective='binary', class_weight='balanced')\n",
    "\n",
    "for train_index, test_index in kf.split(X_matrix):\n",
    "    train_X, train_y = X_matrix[train_index], y_matrix[train_index]\n",
    "    test_X, test_y = X_matrix[test_index], y_matrix[test_index]\n",
    "    gb.fit(train_X, train_y)\n",
    "    lightgmb_predicts[test_index] = gb.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb_predicts = np.zeros(y_matrix.shape)\n",
    "\n",
    "for train_index, test_index in kf.split(X_matrix):\n",
    "    train_X, train_y = X_matrix[train_index], y_matrix[train_index]\n",
    "    test_X, test_y = X_matrix[test_index], y_matrix[test_index]\n",
    "    nb.fit(train_X, train_y)\n",
    "    nb_predicts[test_index] = nb.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 1s - loss: 7.4170 - acc: 0.9983\n",
      "Epoch 2/100\n",
      " - 0s - loss: 6.8875 - acc: 0.9975\n",
      "Epoch 3/100\n",
      " - 3s - loss: 5.8625 - acc: 0.9569\n",
      "Epoch 4/100\n",
      " - 1s - loss: 5.6634 - acc: 0.7434\n",
      "Epoch 5/100\n",
      " - 1s - loss: 5.6994 - acc: 0.6808\n",
      "Epoch 6/100\n",
      " - 1s - loss: 5.6826 - acc: 0.6368\n",
      "Epoch 7/100\n",
      " - 1s - loss: 5.6184 - acc: 0.6068\n",
      "Epoch 8/100\n",
      " - 1s - loss: 5.4913 - acc: 0.5790\n",
      "Epoch 9/100\n",
      " - 1s - loss: 5.3256 - acc: 0.5541\n",
      "Epoch 10/100\n",
      " - 0s - loss: 5.0936 - acc: 0.5289\n",
      "Epoch 11/100\n",
      " - 0s - loss: 4.7437 - acc: 0.5010\n",
      "Epoch 12/100\n",
      " - 1s - loss: 4.4902 - acc: 0.4650\n",
      "Epoch 13/100\n",
      " - 1s - loss: 4.2243 - acc: 0.4272\n",
      "Epoch 14/100\n",
      " - 0s - loss: 3.8848 - acc: 0.3920\n",
      "Epoch 15/100\n",
      " - 0s - loss: 3.3541 - acc: 0.3524\n",
      "Epoch 16/100\n",
      " - 0s - loss: 2.5730 - acc: 0.3345\n",
      "Epoch 17/100\n",
      " - 0s - loss: 2.0299 - acc: 0.4755\n",
      "Epoch 18/100\n",
      " - 0s - loss: 2.2570 - acc: 0.5064\n",
      "Epoch 19/100\n",
      " - 0s - loss: 2.5315 - acc: 0.4624\n",
      "Epoch 20/100\n",
      " - 0s - loss: 2.6852 - acc: 0.4161\n",
      "Epoch 21/100\n",
      " - 0s - loss: 2.6362 - acc: 0.3812\n",
      "Epoch 22/100\n",
      " - 0s - loss: 2.3478 - acc: 0.3595\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.8988 - acc: 0.3432\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.4988 - acc: 0.3823\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.5559 - acc: 0.4158\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.8427 - acc: 0.5235\n",
      "Epoch 27/100\n",
      " - 0s - loss: 2.1189 - acc: 0.5738\n",
      "Epoch 28/100\n",
      " - 0s - loss: 2.1289 - acc: 0.5753\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.9485 - acc: 0.5490\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.7702 - acc: 0.4980\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.6226 - acc: 0.4595\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.4471 - acc: 0.4711\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.3927 - acc: 0.6315\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.6456 - acc: 0.6932\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.6512 - acc: 0.6936\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.4609 - acc: 0.6292\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.2807 - acc: 0.5861\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.2484 - acc: 0.5742\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.3869 - acc: 0.5772\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.4392 - acc: 0.6220\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.3849 - acc: 0.7059\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.4855 - acc: 0.8972\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.5248 - acc: 0.9259\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.2564 - acc: 0.8637\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.1289 - acc: 0.6994\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.2604 - acc: 0.5214\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.2567 - acc: 0.4716\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.3987 - acc: 0.4762\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.2985 - acc: 0.5826\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.2736 - acc: 0.7010\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.2782 - acc: 0.8796\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.3684 - acc: 0.9294\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.1494 - acc: 0.8862\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.9857 - acc: 0.7559\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.0263 - acc: 0.5622\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.1011 - acc: 0.5924\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.1942 - acc: 0.6011\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.1819 - acc: 0.6729\n",
      "Epoch 59/100\n",
      " - 0s - loss: 1.1502 - acc: 0.7567\n",
      "Epoch 60/100\n",
      " - 0s - loss: 1.0345 - acc: 0.8336\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.9886 - acc: 0.8397\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.9833 - acc: 0.7645\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.9770 - acc: 0.6549\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.9046 - acc: 0.7645\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.9887 - acc: 0.8220\n",
      "Epoch 66/100\n",
      " - 0s - loss: 1.1129 - acc: 0.8321\n",
      "Epoch 67/100\n",
      " - 0s - loss: 1.1303 - acc: 0.8023\n",
      "Epoch 68/100\n",
      " - 0s - loss: 1.0508 - acc: 0.7063\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.9739 - acc: 0.6630\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.9374 - acc: 0.6654\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.9183 - acc: 0.6848\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.9686 - acc: 0.7213\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.9937 - acc: 0.7152\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.9412 - acc: 0.6881\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.8694 - acc: 0.6321\n",
      "Epoch 76/100\n",
      " - 0s - loss: 1.1421 - acc: 0.5462\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.9454 - acc: 0.6397\n",
      "Epoch 78/100\n",
      " - 0s - loss: 1.3385 - acc: 0.9415\n",
      "Epoch 79/100\n",
      " - 0s - loss: 1.6818 - acc: 0.9488\n",
      "Epoch 80/100\n",
      " - 0s - loss: 1.4542 - acc: 0.8860\n",
      "Epoch 81/100\n",
      " - 0s - loss: 1.1757 - acc: 0.7569\n",
      "Epoch 82/100\n",
      " - 0s - loss: 1.0571 - acc: 0.6160\n",
      "Epoch 83/100\n",
      " - 0s - loss: 1.4764 - acc: 0.4532\n",
      "Epoch 84/100\n",
      " - 0s - loss: 1.6177 - acc: 0.3762\n",
      "Epoch 85/100\n",
      " - 0s - loss: 1.6200 - acc: 0.3492\n",
      "Epoch 86/100\n",
      " - 0s - loss: 1.1989 - acc: 0.4123\n",
      "Epoch 87/100\n",
      " - 0s - loss: 1.0550 - acc: 0.5410\n",
      "Epoch 88/100\n",
      " - 0s - loss: 1.0175 - acc: 0.7306\n",
      "Epoch 89/100\n",
      " - 0s - loss: 1.2093 - acc: 0.8473\n",
      "Epoch 90/100\n",
      " - 0s - loss: 1.3221 - acc: 0.8879\n",
      "Epoch 91/100\n",
      " - 0s - loss: 1.2178 - acc: 0.8795\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.9894 - acc: 0.8131\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.9047 - acc: 0.6645\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.9548 - acc: 0.5486\n",
      "Epoch 95/100\n",
      " - 0s - loss: 1.3447 - acc: 0.4502\n",
      "Epoch 96/100\n",
      " - 0s - loss: 1.3950 - acc: 0.4738\n",
      "Epoch 97/100\n",
      " - 0s - loss: 1.3653 - acc: 0.5379\n",
      "Epoch 98/100\n",
      " - 0s - loss: 1.3379 - acc: 0.6065\n",
      "Epoch 99/100\n",
      " - 0s - loss: 1.2834 - acc: 0.6526\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.9380 - acc: 0.7103\n",
      "Epoch 1/100\n",
      " - 1s - loss: 1.3048 - acc: 0.8881\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.6434 - acc: 0.2343\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.2904 - acc: 0.2562\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.1449 - acc: 0.5982\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.7504 - acc: 0.9711\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.4648 - acc: 0.9903\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.4130 - acc: 0.9863\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.9277 - acc: 0.9573\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.5082 - acc: 0.8425\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.4019 - acc: 0.6711\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.5171 - acc: 0.5261\n",
      "Epoch 12/100\n",
      " - 0s - loss: 1.7127 - acc: 0.4231\n",
      "Epoch 13/100\n",
      " - 0s - loss: 1.8402 - acc: 0.3750\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.8355 - acc: 0.3728\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.7114 - acc: 0.4118\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.5467 - acc: 0.4895\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.4119 - acc: 0.5852\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.3451 - acc: 0.6816\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.3389 - acc: 0.7779\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.3897 - acc: 0.8620\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.4403 - acc: 0.9072\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.3977 - acc: 0.9113\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.2651 - acc: 0.8753\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.1327 - acc: 0.8012\n",
      "Epoch 25/100\n",
      " - 1s - loss: 1.0961 - acc: 0.6993\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.1474 - acc: 0.6080\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.2002 - acc: 0.5567\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.1717 - acc: 0.5735\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.0715 - acc: 0.6615\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.0257 - acc: 0.7983\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.1214 - acc: 0.9041\n",
      "Epoch 32/100\n",
      " - 1s - loss: 1.1002 - acc: 0.9055\n",
      "Epoch 33/100\n",
      " - 1s - loss: 1.0089 - acc: 0.8305\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.0163 - acc: 0.7204\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.0901 - acc: 0.6183\n",
      "Epoch 36/100\n",
      " - 1s - loss: 1.0687 - acc: 0.6154\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.9995 - acc: 0.7121\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.0005 - acc: 0.8237\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.0787 - acc: 0.8716\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.0707 - acc: 0.8733\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.9970 - acc: 0.8296\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.9728 - acc: 0.7542\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.0129 - acc: 0.6818\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.0185 - acc: 0.6697\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.0010 - acc: 0.6826\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.9814 - acc: 0.7193\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.9663 - acc: 0.7546\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9469 - acc: 0.7769\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.9321 - acc: 0.7785\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.9305 - acc: 0.7553\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.9068 - acc: 0.7965\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.9065 - acc: 0.8052\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.9157 - acc: 0.7798\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.9063 - acc: 0.8069\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.9105 - acc: 0.8408\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.9000 - acc: 0.8069\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.8923 - acc: 0.8162\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.8861 - acc: 0.8637\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.8782 - acc: 0.8416\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.8808 - acc: 0.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      " - 0s - loss: 0.8770 - acc: 0.8265\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.8861 - acc: 0.8308\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.8895 - acc: 0.7774\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.8766 - acc: 0.7917\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.8812 - acc: 0.8435\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.8620 - acc: 0.8300\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8652 - acc: 0.8181\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8562 - acc: 0.8497\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.8625 - acc: 0.8644\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.8662 - acc: 0.8071\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.8575 - acc: 0.8127\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.8634 - acc: 0.8540\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.8473 - acc: 0.8180\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.8504 - acc: 0.8019\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.8388 - acc: 0.8379\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.8562 - acc: 0.8590\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.8496 - acc: 0.7986\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.8491 - acc: 0.7950\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.8449 - acc: 0.8488\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.8297 - acc: 0.8263\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.8321 - acc: 0.8034\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.8258 - acc: 0.8196\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.8315 - acc: 0.8401\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.8344 - acc: 0.7918\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.8272 - acc: 0.8060\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.8345 - acc: 0.8548\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.8201 - acc: 0.8160\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.8248 - acc: 0.7967\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.8154 - acc: 0.8270\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.8293 - acc: 0.8445\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.8228 - acc: 0.7901\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.8260 - acc: 0.7871\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.8144 - acc: 0.8437\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.8110 - acc: 0.8448\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.8105 - acc: 0.8227\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.8095 - acc: 0.8181\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.8062 - acc: 0.8355\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.8084 - acc: 0.8443\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.8090 - acc: 0.8086\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.8076 - acc: 0.8137\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.7983 - acc: 0.8563\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.7432 - acc: 0.2369\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.7294 - acc: 0.3742\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.0983 - acc: 0.9615\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.6726 - acc: 0.9875\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.4934 - acc: 0.9286\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.4827 - acc: 0.8166\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.3997 - acc: 0.7641\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.2176 - acc: 0.7659\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.1226 - acc: 0.7849\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.1230 - acc: 0.7585\n",
      "Epoch 12/100\n",
      " - 1s - loss: 1.1732 - acc: 0.7167\n",
      "Epoch 13/100\n",
      " - 0s - loss: 1.2171 - acc: 0.6717\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.2242 - acc: 0.6410\n",
      "Epoch 15/100\n",
      " - 1s - loss: 1.1810 - acc: 0.6289\n",
      "Epoch 16/100\n",
      " - 1s - loss: 1.0949 - acc: 0.6268\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.9954 - acc: 0.6488\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.9087 - acc: 0.7224\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.9152 - acc: 0.8226\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.9970 - acc: 0.8707\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.0091 - acc: 0.8585\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.9942 - acc: 0.7870\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.0053 - acc: 0.7387\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.9315 - acc: 0.7288\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.8265 - acc: 0.7644\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.8827 - acc: 0.8235\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.9481 - acc: 0.8176\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.9255 - acc: 0.7911\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.8890 - acc: 0.7533\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.8794 - acc: 0.7176\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.8803 - acc: 0.7209\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.8720 - acc: 0.7742\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.8752 - acc: 0.8396\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.8940 - acc: 0.8822\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.8867 - acc: 0.8916\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.8455 - acc: 0.8665\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.8254 - acc: 0.8035\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.8239 - acc: 0.7545\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.8286 - acc: 0.7086\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.7930 - acc: 0.7543\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.8019 - acc: 0.7906\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.7056 - acc: 0.7401\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.7049 - acc: 0.7762\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.7756 - acc: 0.7616\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.8358 - acc: 0.9062\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.8702 - acc: 0.8926\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.9456 - acc: 0.8611\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9141 - acc: 0.8639\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.8494 - acc: 0.8955\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.8230 - acc: 0.8524\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.8262 - acc: 0.7572\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.8462 - acc: 0.7091\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.8654 - acc: 0.7306\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.8505 - acc: 0.7400\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.8099 - acc: 0.7343\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.7733 - acc: 0.7879\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.7656 - acc: 0.8844\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.8056 - acc: 0.9156\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.8000 - acc: 0.8726\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.8025 - acc: 0.8303\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.7636 - acc: 0.8278\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.7683 - acc: 0.8467\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.7504 - acc: 0.8033\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.7637 - acc: 0.7625\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.7489 - acc: 0.7823\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.7399 - acc: 0.8436\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.7352 - acc: 0.8515\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.7452 - acc: 0.8530\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.7436 - acc: 0.8657\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.7353 - acc: 0.8742\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.7288 - acc: 0.8606\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.7303 - acc: 0.8113\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.7365 - acc: 0.7936\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.7287 - acc: 0.8153\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.7366 - acc: 0.8479\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.7292 - acc: 0.8241\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.7378 - acc: 0.8356\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7260 - acc: 0.8819\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7368 - acc: 0.8998\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7180 - acc: 0.8436\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7339 - acc: 0.7999\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7154 - acc: 0.8242\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7429 - acc: 0.8624\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7094 - acc: 0.8410\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7197 - acc: 0.8290\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.7117 - acc: 0.8546\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7112 - acc: 0.8891\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7142 - acc: 0.8904\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7094 - acc: 0.8321\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7201 - acc: 0.8067\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6987 - acc: 0.8468\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.7316 - acc: 0.8805\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6998 - acc: 0.8380\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.7196 - acc: 0.8157\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6975 - acc: 0.8565\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.7119 - acc: 0.8971\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6928 - acc: 0.8576\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.7044 - acc: 0.8255\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6907 - acc: 0.8464\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6964 - acc: 0.8794\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.8084 - acc: 0.8540\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.5992 - acc: 0.6279\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.8736 - acc: 0.5926\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.2567 - acc: 0.9567\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.4795 - acc: 0.9525\n",
      "Epoch 6/100\n",
      " - 1s - loss: 1.2099 - acc: 0.8506\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.0784 - acc: 0.7628\n",
      "Epoch 8/100\n",
      " - 1s - loss: 1.0027 - acc: 0.7479\n",
      "Epoch 9/100\n",
      " - 1s - loss: 1.0145 - acc: 0.7393\n",
      "Epoch 10/100\n",
      " - 1s - loss: 1.1228 - acc: 0.7297\n",
      "Epoch 11/100\n",
      " - 1s - loss: 1.1294 - acc: 0.7319\n",
      "Epoch 12/100\n",
      " - 1s - loss: 1.0147 - acc: 0.7449\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.9066 - acc: 0.7623\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.9024 - acc: 0.7987\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.9768 - acc: 0.8317\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.9986 - acc: 0.8427\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.9562 - acc: 0.7796\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.9011 - acc: 0.7500\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.8983 - acc: 0.7810\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.9376 - acc: 0.8316\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.9551 - acc: 0.8373\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.8790 - acc: 0.8246\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.8552 - acc: 0.8016\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.8723 - acc: 0.8141\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.8931 - acc: 0.8428\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.8884 - acc: 0.8693\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.8578 - acc: 0.8749\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.8419 - acc: 0.8513\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.8549 - acc: 0.8260\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.8590 - acc: 0.8212\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.8468 - acc: 0.8380\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 0.8321 - acc: 0.8652\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.8340 - acc: 0.8834\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.8400 - acc: 0.8763\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.8395 - acc: 0.8522\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.8325 - acc: 0.8391\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.8204 - acc: 0.8527\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.8199 - acc: 0.8759\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.8267 - acc: 0.8851\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.8200 - acc: 0.8777\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.8142 - acc: 0.8616\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.8157 - acc: 0.8561\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.8160 - acc: 0.8679\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.8136 - acc: 0.8875\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.8087 - acc: 0.8952\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.8053 - acc: 0.8843\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.8068 - acc: 0.8681\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.8069 - acc: 0.8622\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.8029 - acc: 0.8715\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.8011 - acc: 0.8861\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.8016 - acc: 0.8914\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.7994 - acc: 0.8832\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.7971 - acc: 0.8728\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.7951 - acc: 0.8731\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.7948 - acc: 0.8818\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.7947 - acc: 0.8881\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.7919 - acc: 0.8848\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.7903 - acc: 0.8777\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.7898 - acc: 0.8773\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.7883 - acc: 0.8852\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.7864 - acc: 0.8905\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.7851 - acc: 0.8872\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.7847 - acc: 0.8793\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.7834 - acc: 0.8772\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.7814 - acc: 0.8839\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.7807 - acc: 0.8920\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.7801 - acc: 0.8927\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.7784 - acc: 0.8866\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.7774 - acc: 0.8828\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.7768 - acc: 0.8852\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.7757 - acc: 0.8887\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.7742 - acc: 0.8896\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.7736 - acc: 0.8879\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.7727 - acc: 0.8887\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.7713 - acc: 0.8909\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.7706 - acc: 0.8906\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.7698 - acc: 0.8882\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.7687 - acc: 0.8869\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.7676 - acc: 0.8898\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.7669 - acc: 0.8941\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.7659 - acc: 0.8940\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.7648 - acc: 0.8906\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.7641 - acc: 0.8897\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.7631 - acc: 0.8919\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.7621 - acc: 0.8945\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.7613 - acc: 0.8946\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.7604 - acc: 0.8938\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.7595 - acc: 0.8941\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.7587 - acc: 0.8952\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.7578 - acc: 0.8948\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.7570 - acc: 0.8940\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.7562 - acc: 0.8953\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.7553 - acc: 0.8967\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.7544 - acc: 0.8965\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.7536 - acc: 0.8957\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.7528 - acc: 0.8961\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.7519 - acc: 0.8977\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.7511 - acc: 0.8995\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.7504 - acc: 0.8985\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.7496 - acc: 0.8953\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.8239 - acc: 0.8953\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.1370 - acc: 0.3959\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.0400 - acc: 0.6920\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.5487 - acc: 0.9828\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.1931 - acc: 0.9706\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.7874 - acc: 0.8183\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.8779 - acc: 0.6214\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.9092 - acc: 0.6465\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.8244 - acc: 0.6723\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.7269 - acc: 0.7094\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.6262 - acc: 0.7907\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.7981 - acc: 0.8752\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.8563 - acc: 0.7911\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.5897 - acc: 0.8194\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.7358 - acc: 0.8883\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.8580 - acc: 0.8914\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.9786 - acc: 0.8708\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.9835 - acc: 0.8576\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.8831 - acc: 0.8590\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.7705 - acc: 0.8735\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.7356 - acc: 0.8840\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.7831 - acc: 0.8601\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.8399 - acc: 0.8254\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.8500 - acc: 0.8021\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.8243 - acc: 0.7924\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.7803 - acc: 0.8004\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.7428 - acc: 0.8231\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.7196 - acc: 0.8594\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.7211 - acc: 0.8943\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.7466 - acc: 0.9113\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.7467 - acc: 0.9070\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.7199 - acc: 0.8855\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.7044 - acc: 0.8594\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.6992 - acc: 0.8473\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.6989 - acc: 0.8559\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.7062 - acc: 0.8717\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.7031 - acc: 0.8810\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.6860 - acc: 0.8856\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.6767 - acc: 0.8869\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.6812 - acc: 0.8857\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.6879 - acc: 0.8916\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.6866 - acc: 0.8996\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.6781 - acc: 0.9055\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.6703 - acc: 0.9012\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.6704 - acc: 0.8886\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.6743 - acc: 0.8768\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.6747 - acc: 0.8724\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.6694 - acc: 0.8765\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.6632 - acc: 0.8886\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.6618 - acc: 0.9011\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.6640 - acc: 0.9072\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.6643 - acc: 0.9046\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.6616 - acc: 0.8984\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.6578 - acc: 0.8935\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.6560 - acc: 0.8921\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.6571 - acc: 0.8927\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.6580 - acc: 0.8925\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.6558 - acc: 0.8907\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.6533 - acc: 0.8905\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.6523 - acc: 0.8953\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.6522 - acc: 0.9030\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.6520 - acc: 0.9093\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.6505 - acc: 0.9099\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.6486 - acc: 0.9040\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.6481 - acc: 0.8955\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.6482 - acc: 0.8917\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.6474 - acc: 0.8939\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.6460 - acc: 0.8998\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.6448 - acc: 0.9044\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.6441 - acc: 0.9057\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.6439 - acc: 0.9043\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.6431 - acc: 0.9036\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.6419 - acc: 0.9039\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.6412 - acc: 0.9040\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.6408 - acc: 0.9021\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.6402 - acc: 0.8997\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.6393 - acc: 0.8992\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.6384 - acc: 0.9017\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.6379 - acc: 0.9059\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.6374 - acc: 0.9082\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.6367 - acc: 0.9076\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.6359 - acc: 0.9045\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.6353 - acc: 0.9026\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.6348 - acc: 0.9032\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.6342 - acc: 0.9052\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.6336 - acc: 0.9066\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.6330 - acc: 0.9076\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.6325 - acc: 0.9078\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.6319 - acc: 0.9085\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.6313 - acc: 0.9090\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6306 - acc: 0.9085\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6301 - acc: 0.9073\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6296 - acc: 0.9068\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6290 - acc: 0.9072\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6284 - acc: 0.9087\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6278 - acc: 0.9106\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6273 - acc: 0.9114\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6267 - acc: 0.9106\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.6261 - acc: 0.9092\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.6256 - acc: 0.9089\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "nn = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "nn_predicts = np.zeros(y_matrix.shape)\n",
    "\n",
    "for train_index, test_index in kf.split(X_matrix):\n",
    "    train_X, train_y = X_matrix[train_index], y_matrix[train_index]\n",
    "    test_X, test_y = X_matrix[test_index], y_matrix[test_index]\n",
    "    \n",
    "    cws = {i: w for i, w in enumerate(class_weight.compute_class_weight('balanced', np.unique(train_y), train_y))}\n",
    "    \n",
    "    nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    nn.fit(train_X, train_y, batch_size=len(train_X), epochs=100, class_weight=cws, verbose=2)\n",
    "    nn_predicts[test_index] = nn.predict_classes(test_X).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(true_y, pred_y):\n",
    "    print(\n",
    "        'Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF-Score: {}\\nMSE: {}'.format(\n",
    "        metrics.accuracy_score(true_y, pred_y),\n",
    "        metrics.precision_score(true_y, pred_y),\n",
    "        metrics.recall_score(true_y, pred_y),\n",
    "        metrics.f1_score(true_y, pred_y),\n",
    "        metrics.mean_squared_error(true_y, pred_y),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7897290369827902\n",
      "Precision: 0.00398940993000399\n",
      "Recall: 0.5213270142180095\n",
      "F-Score: 0.007918226317304926\n",
      "MSE: 0.21027096301720982\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_matrix, lr_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия - самый низкий precision, видимо слишком много у нас FP (судя по accuracy, если пренебречь долей положительного класса, то 20%), зато неплохой recall! Все-таки мы пытаемся балансировать классы, и регрессия пытается половину положительных детектировать правильно, хоть и вместе с этим 20% нулевых записываются в положительные.\n",
    "\n",
    "Precision всего в 2 раза большое, чем доля положительного класса в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9984132796289515\n",
      "Precision: 0.5789473684210527\n",
      "Recall: 0.052132701421800945\n",
      "F-Score: 0.09565217391304347\n",
      "MSE: 0.001586720371048456\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_matrix, rf_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой recall и хороший precision - лес почти каждый объект старается отнести к 0 классу, даже 95% положительных объектов туда отнес, и только 5% положительных объектов он отнес правильно, и столько же к 1 классу он отнес элементов 0 класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9972232393506651\n",
      "Precision: 0.23529411764705882\n",
      "Recall: 0.3222748815165877\n",
      "F-Score: 0.27199999999999996\n",
      "MSE: 0.002776760649334798\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_matrix, lightgmb_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая высокая F-мера! Несмотря на то, что по другим категориям другие классификаторы его обошли, F-мера в данной ситуации более репрезентативна, поэтому пока LGBM побеждает "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9918833150250214\n",
      "Precision: 0.03691639522258415\n",
      "Recall: 0.16113744075829384\n",
      "F-Score: 0.06007067137809187\n",
      "MSE: 0.00811668497497864\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_matrix, nb_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как логистическая регрессия, только хуже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8870758574392774\n",
      "Precision: 0.008554492792671427\n",
      "Recall: 0.6018957345971564\n",
      "F-Score: 0.0168692302583516\n",
      "MSE: 0.11292414256072257\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_matrix, nn_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый большой recall, точность в 5 раз выше концентрации 1 класса, топ-2 F-мера. Нейросети - вторые после LGBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
